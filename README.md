# Image Captioning - Group 2 - UAB Deep Learning Course
Our main proposal for this project is to be able to train an image captioning model based on the [Flickr 8k Dataset on Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr8k).

*Click on Image to open Kaggle Dataset page:* 

[<img src="docs/site-logo.svg" width="120"/>](https://www.kaggle.com/datasets/adityajn105/flickr8k)

## General Project Structure
You must create as many folders as you consider. You can use the proposed structure or replace it by the one in the base code that you use as starting point. Do not forget to add Markdown files as needed to explain well the code and how to use it.

## How to run the code
Explain dependencies, what does each file in the repository contain, etc.

## Code Structure
Explain what does each script do.
## Example Code
The given code is a simple CNN example training on the MNIST dataset. It shows how to set up the [Weights & Biases](https://wandb.ai/site)  package to monitor how your network is learning, or not.

Before running the code you have to create a local environment with conda and activate it. The provided [environment.yml](https://github.com/DCC-UAB/XNAP-Project/environment.yml) file has all the required dependencies. Run the following command: ``conda env create --file environment.yml `` to create a conda environment with all the required dependencies and then activate it:
```
conda activate imgcaption
```

To run the example code:
```
python main.py
```



## Contributors
* **Miguel Moral Hernández - miguel.moral@autonoma.cat**
* **Àlex Sànchez Zurita - alex.sanchezz@autonoma.cat**
* **Pol Medina Arévalo - pol.medina@autonoma.cat**

Xarxes Neuronals i Aprenentatge Profund,
Grau de Intel·ligència Artificial, 
UAB, 2023
