{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from model import EncoderCNN2DecoderRNN\n",
    "from get_loader_v2_train_val_test import get_loader, get_length_vocab, get_pad_index, get_vocab, show_image\n",
    "\n",
    "from get_loader_v2_train_val_test import Vocabulary\n",
    "from get_loader_v2_train_val_test import Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Images/'\n",
    "captions_file = 'data/captions.txt'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_captions = pd.read_csv(captions_file)\n",
    "unique_images = df_captions['image'].unique()\n",
    "train_images, test_images = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "train_images, val_images = train_test_split(train_images, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df_captions[df_captions['image'].isin(train_images)]\n",
    "val_df = df_captions[df_captions['image'].isin(val_images)]\n",
    "test_df = df_captions[df_captions['image'].isin(test_images)]\n",
    "\n",
    "lenght_train_df = get_length_vocab(data_dir=data_dir, dataframe=train_df, transform=transform)\n",
    "lenght_val_df = get_length_vocab(data_dir=data_dir, dataframe=val_df, transform=transform)\n",
    "lenght_test_df = get_length_vocab(data_dir=data_dir, dataframe=test_df, transform=transform)\n",
    "\n",
    "pad_index = get_pad_index(data_dir=data_dir, dataframe=train_df, transform=transform)\n",
    "\n",
    "vocab_train_df = get_vocab(data_dir=data_dir, dataframe=train_df, transform=transform)\n",
    "vocab_val_df = get_vocab(data_dir=data_dir, dataframe=val_df, transform=transform)\n",
    "vocab_test_df = get_vocab(data_dir=data_dir, dataframe=test_df, transform=transform)\n",
    "\n",
    "    \n",
    "\n",
    "# Create train, validation, and test data loaders\n",
    "train_dataloader = get_loader(data_dir=data_dir, dataframe=train_df, transform=transform)\n",
    "val_dataloader = get_loader(data_dir=data_dir, dataframe=val_df, transform=transform)\n",
    "test_dataloader = get_loader(data_dir=data_dir, dataframe=test_df, transform=transform)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True) \n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1] # To extract the features of Rsenet from the last layer before the Softmax is applied\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features,embed_size) \n",
    "        \n",
    "    def forward(self,images):\n",
    "        features = self.resnet(images)\n",
    "#         print(f\"resenet features shape - {features.shape}\")\n",
    "        features = features.view(features.size(0),-1)\n",
    "#         print(f\"resenet features viewed shape - {features.shape}\")\n",
    "        features = self.embed(features)\n",
    "#         print(f\"resenet features embed shape - {features.shape}\")\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fcn = nn.Linear(hidden_size,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self,features, captions):\n",
    "        # vectorize the caption\n",
    "#         print(f\"captions - {captions[:,:-1]}\")\n",
    "#         print(f\"caption shape - {captions[:,:-1].shape}\")\n",
    "        embeds = self.embedding(captions[:,:-1])\n",
    "#         print(f\"shape of embeds - {embeds.shape}\")\n",
    "        # concat the features and captions\n",
    "#         print(f\"features shape - {features.shape}\")\n",
    "#         print(f\"features unsqueeze at index 1 shape - {features.unsqueeze(1).shape}\")\n",
    "        x = torch.cat((features.unsqueeze(1),embeds),dim=1)\n",
    "#         print(f\"shape of x - {x.shape}\")\n",
    "        x,_ = self.lstm(x)\n",
    "#         print(f\"shape of x after lstm - {x.shape}\")\n",
    "        x = self.fcn(x)\n",
    "#         print(f\"shape of x after fcn - {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None):\n",
    "    # Inference part\n",
    "    # Given the image features generate the captions\n",
    "    \n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output,hidden = self.lstm(inputs,hidden)\n",
    "            output = self.fcn(output)\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "# resenet features shape - torch.Size([4, 2048, 1, 1])\n",
    "# resenet features viewed shape - torch.Size([4, 2048])\n",
    "# resenet features embed shape - torch.Size([4, 400])\n",
    "# caption shape - torch.Size([4, 14])\n",
    "# shape of embeds - torch.Size([4, 14, 400])\n",
    "# features shape - torch.Size([4, 400])\n",
    "# features unsqueeze at index 1 shape - torch.Size([4, 1, 400])\n",
    "# shape of x - torch.Size([4, 15, 400])\n",
    "# shape of x after lstm - torch.Size([4, 15, 512])\n",
    "# shape of x after fcn - torch.Size([4, 15, 2994])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 400\n",
    "hidden_size = 512\n",
    "vocab_size = lenght_train_df\n",
    "num_layers = 2\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Miguel\\anaconda3\\envs\\Pytorch_2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Miguel\\anaconda3\\envs\\Pytorch_2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# initialize model, loss etc\n",
    "model = EncoderDecoder(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):   \n\u001b[0;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m idx, (image, captions) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39miter\u001b[39m(train_dataloader)):\n\u001b[1;32m----> 6\u001b[0m         image,captions \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mto(device),captions\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         \u001b[39m# Zero the gradients.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "print_every = 2000\n",
    "\n",
    "for epoch in range(1,num_epochs+1):   \n",
    "    for idx, (image, captions) in enumerate(iter(train_dataloader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs = model(image, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "            \n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(train_dataloader)\n",
    "                img,_ = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                print(f\"features shape - {features.shape}\")\n",
    "                caps = model.decoder.generate_caption(features.unsqueeze(0),vocab=vocab_train_df)\n",
    "                caption = ' '.join(caps)\n",
    "                print(caption)\n",
    "                show_image(img[0],title=caption)\n",
    "                \n",
    "            model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
