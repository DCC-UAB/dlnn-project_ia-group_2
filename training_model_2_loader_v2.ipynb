{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from model import EncoderCNN2DecoderRNN\n",
    "from get_loader_v2_train_val_test import get_loader\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1cedcd8c670>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'data/Images/'\n",
    "captions_file = 'data/captions.txt'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_captions = pd.read_csv(captions_file)\n",
    "unique_images = df_captions['image'].unique()\n",
    "train_images, test_images = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "train_images, val_images = train_test_split(train_images, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df_captions[df_captions['image'].isin(train_images)]\n",
    "val_df = df_captions[df_captions['image'].isin(val_images)]\n",
    "test_df = df_captions[df_captions['image'].isin(test_images)]\n",
    "\n",
    "# Create train, validation, and test data loaders\n",
    "train_dataloader = get_loader(data_dir=data_dir, dataframe=train_df, transform=transform)\n",
    "val_dataloader = get_loader(data_dir=data_dir, dataframe=val_df, transform=transform)\n",
    "test_dataloader = get_loader(data_dir=data_dir, dataframe=test_df, transform=transform)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True) \n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1] # To extract the features of Rsenet from the last layer before the Softmax is applied\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features,embed_size) \n",
    "        \n",
    "    def forward(self,images):\n",
    "        features = self.resnet(images)\n",
    "#         print(f\"resenet features shape - {features.shape}\")\n",
    "        features = features.view(features.size(0),-1)\n",
    "#         print(f\"resenet features viewed shape - {features.shape}\")\n",
    "        features = self.embed(features)\n",
    "#         print(f\"resenet features embed shape - {features.shape}\")\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fcn = nn.Linear(hidden_size,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self,features, captions):\n",
    "        # vectorize the caption\n",
    "#         print(f\"captions - {captions[:,:-1]}\")\n",
    "#         print(f\"caption shape - {captions[:,:-1].shape}\")\n",
    "        embeds = self.embedding(captions[:,:-1])\n",
    "#         print(f\"shape of embeds - {embeds.shape}\")\n",
    "        # concat the features and captions\n",
    "#         print(f\"features shape - {features.shape}\")\n",
    "#         print(f\"features unsqueeze at index 1 shape - {features.unsqueeze(1).shape}\")\n",
    "        x = torch.cat((features.unsqueeze(1),embeds),dim=1)\n",
    "#         print(f\"shape of x - {x.shape}\")\n",
    "        x,_ = self.lstm(x)\n",
    "#         print(f\"shape of x after lstm - {x.shape}\")\n",
    "        x = self.fcn(x)\n",
    "#         print(f\"shape of x after fcn - {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None):\n",
    "    # Inference part\n",
    "    # Given the image features generate the captions\n",
    "    \n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output,hidden = self.lstm(inputs,hidden)\n",
    "            output = self.fcn(output)\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "# resenet features shape - torch.Size([4, 2048, 1, 1])\n",
    "# resenet features viewed shape - torch.Size([4, 2048])\n",
    "# resenet features embed shape - torch.Size([4, 400])\n",
    "# caption shape - torch.Size([4, 14])\n",
    "# shape of embeds - torch.Size([4, 14, 400])\n",
    "# features shape - torch.Size([4, 400])\n",
    "# features unsqueeze at index 1 shape - torch.Size([4, 1, 400])\n",
    "# shape of x - torch.Size([4, 15, 400])\n",
    "# shape of x after lstm - torch.Size([4, 15, 512])\n",
    "# shape of x after fcn - torch.Size([4, 15, 2994])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m embed_size \u001b[39m=\u001b[39m \u001b[39m400\u001b[39m\n\u001b[0;32m      3\u001b[0m hidden_size \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m----> 4\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_df\u001b[39m.\u001b[39;49mvocab)\n\u001b[0;32m      5\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Miguel\\anaconda3\\envs\\Pytorch_2\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 400\n",
    "hidden_size = 512\n",
    "vocab_size = len(train_df.vocab)\n",
    "num_layers = 2\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
